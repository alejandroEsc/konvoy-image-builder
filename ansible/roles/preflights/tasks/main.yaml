---
  - name: check for supported OS
    fail:
      msg: "{{ configured_ansible_distribution }} {{ ansible_distribution_major_version }} is not supported. The supported OSes - CentOS 7/8, RHEL 7/8, Suse 15, SLES 15, Ubuntu 16 (xenial) 18 (bionic) or Debian 9 (stretch)"
    failed_when: not (
                 (configured_ansible_distribution == "CentOS" and ansible_distribution_major_version|int in [7, 8])
                 or (configured_ansible_distribution == "RedHat" and ansible_distribution_major_version|int in [7, 8])
                 or (configured_ansible_distribution == 'Ubuntu' and ((ansible_distribution_major_version|int == 16) or (ansible_distribution_major_version|int == 18)))
                 or (configured_ansible_distribution == 'Debian' and ((ansible_distribution_major_version|int == 9) or (ansible_distribution_major_version|int == 10)))
                 or (ansible_distribution == 'Suse' and ansible_distribution_major_version|int == 15)
                 or (ansible_distribution == 'SLES' and ansible_distribution_major_version|int == 15)
                 or (ansible_distribution == 'openSUSE Leap' and ansible_distribution_major_version|int == 15)
                 )

# TODO how to determine which ansible_mounts to use in docker?
#  - name: fail when disk utilization more than 90%
#    command: /bin/true
#    changed_when: false
#    failed_when: ((ansible_mounts[0].size_total - ansible_mounts[0].size_available)/ansible_mounts[0].size_total) > 0.9

  - name: check required OS package registries are accessible for rpm
    uri:
      url: "{{item}}"
      status_code: 200
    with_items:
      - "{{kubernetes_rpm_repository_url}}/repodata/repomd.xml"
      - "{{kubernetes_rpm_gpg_key_url}}"
      - "{{docker_rpm_container_selinux_package_url}}"
      - "{{docker_rpm_container_selinux_gpg_key_url}}"
      - "{{docker_rpm_repository_url}}/repodata/repomd.xml"
      - "{{docker_rpm_gpg_key_url}}"
      - "{{libnvidia_container_rpm_repository_package_url}}/repodata/repomd.xml"
      - "{{libnvidia_container_gpg_key_url}}"
      - "{{nvidia_container_runtime_rpm_repository_package_url}}/repodata/repomd.xml"
      - "{{nvidia_container_runtime_gpg_key_url}}"
    register: result
    until: result is success
    retries: 5
    delay: 3
    when:
      - package_versions.enable_repository_installation
      - ansible_os_family == 'RedHat'
      - ansible_distribution_major_version|int < 8
      - rpms_tar_file == ''

  - name: check required OS package registries are accessible for rpm (rhel8 only)
    uri:
      url: "{{item}}"
      status_code: 200
    with_items:
      - "{{kubernetes_rpm_repository_url}}/repodata/repomd.xml"
      - "{{kubernetes_rpm_gpg_key_url}}"
      - "{{docker_rpm_repository_url}}/repodata/repomd.xml"
      - "{{docker_rpm_gpg_key_url}}"
      - "{{libnvidia_container_rpm_repository_package_url}}/repodata/repomd.xml"
      - "{{libnvidia_container_gpg_key_url}}"
      - "{{nvidia_container_runtime_rpm_repository_package_url}}/repodata/repomd.xml"
      - "{{nvidia_container_runtime_gpg_key_url}}"
    register: result
    until: result is success
    retries: 5
    delay: 3
    when:
      - package_versions.enable_repository_installation
      - ansible_os_family == 'RedHat'
      - ansible_distribution_major_version|int == 8
      - rpms_tar_file == ''

  - name: check required OS package registries are accessible for deb
    uri:
      url: "{{item}}"
      status_code: 200
    with_items:
      - "{{kubernetes_deb_repository_url}}"
      - "{{kubernetes_deb_gpg_key_url}}"
      - "{{docker_deb_repository_url}}/dists/{{docker_deb_release_name}}/{{ docker_deb_component }}/binary-amd64/Packages"
      - "{{docker_deb_gpg_key_url}}"
      - "{{libnvidia_container_deb_repository_url}}/repo.conf"
      - "{{libnvidia_container_gpg_key_url}}"
      - "{{nvidia_container_runtime_deb_package_url}}/repo.conf"
      - "{{nvidia_container_runtime_gpg_key_url}}"
    register: result
    until: result is success
    retries: 5
    delay: 3
    when:
      - package_versions.enable_repository_installation
      - ansible_os_family == 'Debian'
      - debs_tar_file == ''

  - name: check required OS package registries are accessible for zypper
    uri:
      url: "{{item}}"
      status_code: 200
    with_items:
      - "{{kubernetes_zypper_repository_url}}/repodata/repomd.xml"
      - "{{kubernetes_zypper_gpg_key_url}}"
      - "{{docker_zypper_repository_url}}/repodata/repomd.xml"
      - "{{docker_zypper_gpg_key_url}}"
    register: result
    until: result is success
    retries: 5
    delay: 3
    when:
      - package_versions.enable_repository_installation
      - ansible_os_family == 'Suse'

  # kubernetes checks /proc/swaps lines > 1
  - name: check memory swap is disabled
    command: cat /proc/swaps
    register: memory_swaps
    failed_when:
      - memory_swaps is defined
      - memory_swaps.rc is defined
      - (memory_swaps.rc != 0 or (memory_swaps.stdout_lines is defined and memory_swaps.stdout_lines|length > 1))
    changed_when: False
    when: spec.kubernetes.preflightChecks is not defined or
          spec.kubernetes.preflightChecks.errorsToIgnore is not defined or
          not (
            'all' in spec.kubernetes.preflightChecks.errorsToIgnore or
            'swap' in spec.kubernetes.preflightChecks.errorsToIgnore
          )

  - name: check control-plane hostnames are unique
    fail:
      msg: "Control-plane machines in the cluster must have unique hostnames, as reported by the 'hostname' command."
    failed_when: (control_plane_hostnames | length) != (control_plane_hostnames | unique | length)
    when: "'control-plane' in group_names"

  - name: check worker hostnames are unique
    fail:
      msg: "Worker machines in the cluster must have unique hostnames, as reported by the 'hostname' command."
    failed_when: (worker_hostnames | length) != (worker_hostnames | unique | length)
    when: "'control-plane' not in group_names"

  - name: check node IPs do not overlap with serviceSubnet
    fail:
      msg: The service-subnet {{ service_subnet }} cannot overlap with any of the IPs of the hosts {{ inventory_hostname }}.
    failed_when:
      - inventory_hostname | ipaddr(service_subnet)
    run_once: true

  - name: check node IPs do not overlap with podSubnet
    fail:
      msg: The pod-subnet cannot overlap with any of the IPs of the hosts.
    failed_when:
      - inventory_hostname | ipaddr(pod_subnet)
    run_once: true

  - name: check if kubeadm has already run
    stat:
      path: "/var/lib/kubelet/config.yaml"
    register: kubeadm_already_run

  - name: check files and directories do not yet exist
    stat:
      path: "{{ item }}"
    register: result
    with_items:
      - "/etc/kubernetes/manifests/kube-apiserver.yaml"
      - "/etc/kubernetes/manifests/kube-controller-manager.yaml"
      - "/etc/kubernetes/manifests/kube-scheduler.yaml"
      - "/etc/kubernetes/manifests/etcd.yaml"
      - "/etc/kubernetes/manifests/keepalived.yaml"
      - "/var/lib/etcd/member"
    when:
      - "'control-plane' in group_names"
      - not kubeadm_already_run.stat.exists
    failed_when:
      - result.stat.exists == True
    changed_when: False

  - name: check if /var/lib/etcd/ directory is empty
    find:
      paths: "/var/lib/etcd/"
      file_type: any
    register: etcd_dir
    failed_when: etcd_dir.matched > 0
    changed_when: etcd_dir.matched > 0
    when:
      - "'control-plane' in group_names"
      - not kubeadm_already_run.stat.exists

  - name: check required ports for control-plane are unused
    wait_for:
      port: "{{ item }}"
      state: stopped
      timeout: 1
    with_items:
      - "{{ apiserver.secure_port }}"  # kube-apiserver --secure-port
      - 10250 # kubelet --port
      - 10248 # kubelet --healthz-port
      - 10249 # kube-proxy --metrics-bind-address
      - 10256 # kube-proxy --healthz-port
      - 10257 # kube-controller-manager --secure-port
      - 10259 # kube-scheduler --secure-port
      - 2379  # etcd client
      - 2380  # etcd peer
      - 9091  # calico-node felix (used for metrics)
      - 9092  # calico-node bird (used for metrics)
      - 9099  # calico-node felix (used for liveness)
    when:
      - "'control-plane' in group_names"
      - not kubeadm_already_run.stat.exists
    changed_when: False

  - name: check required ports for nodes are unused
    wait_for:
      port: "{{ item }}"
      state: stopped
      timeout: 1
    with_items:
      - 10250 # kubelet --port
      - 10248 # kubelet --healthz-port
      - 10249 # kube-proxy --metrics-bind-address
      - 10256 # kube-proxy --healthz-port
      - 9091  # calico-node felix (used for metrics)
      - 9092  # calico-node bird (used for metrics)
      - 9099  # calico-node felix (used for liveness)
      - 5473  # calico-typha (used for syncserver)
      - 9093  # calico-typha (used for metrics)
    when:
      - "'control-plane' not in group_names"
      - not kubeadm_already_run.stat.exists
    changed_when: False

  - name: check required bird-related ports for nodes are unused
    wait_for:
      port: "{{ item }}"
      state: stopped
      timeout: 1
    with_items:
      - 9092  # calico-node bird_exporter (used for metrics)
      - 179   # calico-node bird (used for BGP)
    when:
      - not kubeadm_already_run.stat.exists
      - "calico_encapsulation != 'vxlan'"
    changed_when: False

  - name: verify control-plane to control-plane connectivity
    command: ping -c 2 {{ item }}
    with_items: "{{ groups['control-plane'] }}"
    when: "'control-plane' in group_names"
    changed_when: False

  - name: verify node to control-plane connectivity
    command: ping -c 2 {{ item }}
    with_items: "{{ groups['control-plane'] }}"
    when: "'node' in group_names"
    changed_when: False

  # Every node should be able to reach all other nodes.
  # Use a random sampling of nodes to avoid quadratic complexity.
  - name: verify node to node connectivity with random sample
    command: ping -c 2 {{ item }}
    with_random_choice: "{{ groups['node'] | map('extract', hostvars, 'inventory_hostname') | list }}"
    when: "'node' in group_names"
    changed_when: False


  # Need to properly handle larger clusters
  # Consider forks - where each task runs in some batch( default 20) and serial where each play is split into batches
  # Hardcoding a specific node will not work as they may be run in a different Ansible batches
  # The `ansible_date_time` var is collected during fact gathering
  # This task assumes that the fact gathering runs around the same time
  # So adding an extra second of allowance for each batch of forks that will run
  - name: compare time is within 30 seconds of the first node in this batch
    fail:
      msg: "The time difference between this node and the first node in this batch is greater than 30 seconds"
    failed_when: ((ansible_date_time.epoch | int) - (hostvars[ansible_play_batch[0]]['ansible_date_time']['epoch'] | int)) | abs > ((30 + (ansible_play_batch|length / ansible_forks)) | int)

  - block:
    - name: check if keepalived is already configured
      stat:
        path: "/etc/kubernetes/manifests/keepalived.yaml"
      delegate_to: "{{ groups['control-plane'][0] }}"
      register: keepalived_file

    - block:
      - name: check reachability to keepalived vip
        command: ping -c 2 {{ keepalived.virtual_ip }}
        changed_when: False
      when:
        - keepalived_file.stat.exists

    - block:
      - name: check if keepalived vip is already present
        command: ping -c 2 {{ keepalived.virtual_ip }}
        register: pingout
        changed_when: pingout.rc == 0
        failed_when: pingout.rc == 0

      - block:
        - block:
          - name: detect a keepalived interface
            shell: ip route get "{{ keepalived.virtual_ip }}" | grep -Po '(?<=(dev )).*(?= src| proto)'
            register: keepalived_intf

          - name: set keepalived interface
            set_fact:
               keepalived:
                 interface: "{{ keepalived_intf.stdout }}"
            when: keepalived.interface == ""

          - name: assign keepalived vip for reachability check
            command: ip addr add {{ keepalived.virtual_ip }} dev {{ keepalived.interface }}

          when: inventory_hostname == groups['control-plane'][0]

        - name: check reachability to keepalived vip
          command: ping -c 2 {{ keepalived.virtual_ip }}
          changed_when: False

        always:
          - name: remove keepalived vip
            command: ip addr del {{ keepalived.virtual_ip }} dev {{ keepalived.interface }}
            when: inventory_hostname == groups['control-plane'][0]

      when:
        - not keepalived_file.stat.exists
    when:
      - keepalived.enabled

  - name: check if /etc/konvoy-marker.yaml exists
    stat:
      path: "/etc/konvoy-marker.yaml"
    register: marker_file

  - name: detect persistent volume disks in /mnt/disks
    find:
      paths: '/mnt/disks/'
      file_type: directory
    register: pvd
    when: not marker_file.stat.exists

  - name: check for old persistent volumes in /mnt/disks/*/
    find:
      paths: "{{ pvd.files | map(attribute='path') | list }}"
      file_type: any
      excludes:
        - 'lost+found'
    register: old_pvs
    failed_when: old_pvs.matched > 0
    changed_when: old_pvs.matched > 0
    when:
      - not marker_file.stat.exists
      - "'node' in group_names"

  - block:
    - name: collect OS disk usage on the root volume
      shell: df -hl | awk '/\/$/ { sum+=$5 } END { print sum }'
      register: os_root_vol_usage

    - name: ensure disk usage on the root volume is less than {{ preflight.disk_usage_threshold }}%
      assert:
        that:
           - os_root_vol_usage.stdout|int < {{ preflight.disk_usage_threshold }}
        msg: Disk space has reached {{ preflight.disk_usage_threshold }}% threshold

    - name: collect OS disk allocated size on the root volume
      shell: df -hl | awk '/\/$/ { sum+=$2 } END { print sum }'
      register: os_root_vol_size

    - name: ensure disk allocated size on the root volume is equal or greater than {{ preflight.disk_size_threshold }}G
      ignore_errors: True
      when:
        - os_root_vol_size.stdout|int < preflight.disk_size_threshold
      fail:
        msg: "WARNING: Disk size {{ os_root_vol_size.stdout|int }}G is less than {{ preflight.disk_size_threshold }}G"

    - name: ensure minimum allocated CPU cores for all the nodes
      assert:
        that:
          - ansible_processor_vcpus >= {{ preflight.minimum_cpu_cores }}
        msg: CPU core allocation {{ ansible_processor_vcpus }} is less than required {{ preflight.minimum_cpu_cores }}

    - name: ensure minimum allocated memory for control plane nodes
      assert:
        that:
          - ansible_memtotal_mb > {{ preflight.minimum_memory_control_plane_mb|int }}
        msg: Memory allocation {{ ansible_memtotal_mb }}MB is less than required {{ preflight.minimum_memory_control_plane_mb|int }}MB for control plane nodes
      when: "'control-plane' in group_names"

    - name: ensure minimum allocated memory for worker nodes
      assert:
        that:
          - ansible_memtotal_mb > {{ preflight.minimum_memory_worker_mb }}
        msg: Memory allocation {{ ansible_memtotal_mb }}MB is less than required {{ preflight.minimum_memory_worker_mb }}MB for workers
      when: "'control-plane' not in group_names"

    when:
      - provisioner is defined and provisioner != 'docker'
      - "'bastion' not in group_names"
